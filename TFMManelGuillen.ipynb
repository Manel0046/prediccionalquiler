{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dkv0wXJ1OsG4"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Subimos el CSV exportado desde R\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Cargamos el CSV\n",
        "df = pd.read_csv(\"Madrid_limpio.csv\")\n",
        "\n",
        "# Vistazo general\n",
        "df.shape\n",
        "df.head()\n",
        "df.columns\n"
      ],
      "metadata": {
        "id": "sM6ESrYQO4yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# Convertimos lat/lon en geometr√≠a\n",
        "gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.LONGITUDE, df.LATITUDE), crs=\"EPSG:4326\")\n",
        "\n",
        "# Visualizamos los primeros puntos\n",
        "gdf.head()\n"
      ],
      "metadata": {
        "id": "HUHktEEpO-X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üß† 1. Preparamos los datos para modelar\n",
        "import numpy as np\n",
        "\n",
        "df_model = df.copy()\n",
        "df_model[\"LOG_PRICE\"] = np.log10(df_model[\"PRICE\"])  # Transformaci√≥n de la variable objetivo\n",
        "\n",
        "# üß± 2. Convertimos variables categ√≥ricas a dummies\n",
        "df_model = pd.get_dummies(df_model, columns=[\"FLOOR_CAT\", \"TIPO_CONSTRUCCION\"], drop_first=False)\n",
        "\n",
        "# ‚ú® 3. Selecci√≥n de variables predictoras\n",
        "features = [\n",
        "    'CONSTRUCTEDAREA', 'ROOMNUMBER', 'BATHNUMBER',\n",
        "    'HASLIFT', 'HASTERRACE', 'HASAIRCONDITIONING',\n",
        "    'HASGARDEN', 'HASSWIMMINGPOOL', 'DISTANCE_TO_CITY_CENTER', 'ANTIGUEDAD'\n",
        "]\n",
        "\n",
        "# A√±adimos dummies\n",
        "dummies = [col for col in df_model.columns if col.startswith(\"FLOOR_CAT_\") or col.startswith(\"TIPO_CONSTRUCCION_\")]\n",
        "features += dummies\n",
        "\n",
        "X = df_model[features]\n",
        "y = df_model[\"LOG_PRICE\"]\n",
        "\n",
        "# üîç 4. Eliminamos cualquier fila con valores nulos\n",
        "model_data = pd.concat([X, y], axis=1)\n",
        "model_data_clean = model_data.dropna()\n",
        "\n",
        "X_clean = model_data_clean.drop(columns=[\"LOG_PRICE\"])\n",
        "y_clean = model_data_clean[\"LOG_PRICE\"]\n",
        "\n",
        "# üîÄ 5. Divisi√≥n train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.2, random_state=42)\n",
        "\n",
        "# ü§ñ 6. Entrenamiento del modelo\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# üìà 7. Evaluaci√≥n\n",
        "y_pred = model.predict(X_test)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "rmse, r2"
      ],
      "metadata": {
        "id": "vbRJzIJ9PCuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Recuperamos predicciones y las convertimos de log10 a escala original\n",
        "y_pred_real = 10 ** y_pred\n",
        "y_test_real = 10 ** y_test\n",
        "\n",
        "# Gr√°fico de dispersi√≥n: precio real vs. predicho\n",
        "plt.figure(figsize=(7, 6))\n",
        "sns.scatterplot(x=y_test_real, y=y_pred_real, alpha=0.3)\n",
        "plt.plot([y_test_real.min(), y_test_real.max()], [y_test_real.min(), y_test_real.max()], 'r--')  # l√≠nea de identidad\n",
        "plt.xlabel(\"Precio real (‚Ç¨)\")\n",
        "plt.ylabel(\"Precio predicho (‚Ç¨)\")\n",
        "plt.title(\"Precio real vs. predicho (modelo lineal)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Wox4HKn4PGF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "polygons = gpd.read_file(\"Madrid_Polygons.shp\")\n",
        "# Mostramos las primeras columnas\n",
        "polygons.head()\n",
        "# Uni√≥n espacial usando LOCATIONN\n",
        "gdf_zonas = gpd.sjoin(gdf, polygons[[\"LOCATIONN\", \"geometry\"]], how=\"left\", predicate=\"within\")\n",
        "\n",
        "# Guardamos el resultado\n",
        "gdf_zonas.drop(columns=\"geometry\").to_csv(\"/content/Madrid_limpio_con_zona.csv\", index=False)\n",
        "\n",
        "gdf_zonas[['LOCATIONN']].value_counts().head(10)\n"
      ],
      "metadata": {
        "id": "EonBCpp-jn3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el CSV con LOCATIONN\n",
        "df = pd.read_csv(\"/content/Madrid_limpio_con_zona.csv\")\n",
        "\n",
        "# Transformamos y preparamos los datos\n",
        "df_model = df.copy()\n",
        "df_model[\"LOG_PRICE\"] = np.log10(df_model[\"PRICE\"])\n",
        "df_model = pd.get_dummies(df_model, columns=[\"FLOOR_CAT\", \"TIPO_CONSTRUCCION\", \"LOCATIONN\"], drop_first=True)\n",
        "\n",
        "# Variables predictoras\n",
        "features = [\n",
        "    'CONSTRUCTEDAREA', 'ROOMNUMBER', 'BATHNUMBER',\n",
        "    'HASLIFT', 'HASTERRACE', 'HASAIRCONDITIONING',\n",
        "    'HASGARDEN', 'HASSWIMMINGPOOL', 'DISTANCE_TO_CITY_CENTER', 'ANTIGUEDAD'\n",
        "]\n",
        "features += [col for col in df_model.columns if col.startswith(\"FLOOR_CAT_\")]\n",
        "features += [col for col in df_model.columns if col.startswith(\"TIPO_CONSTRUCCION_\")]\n",
        "features += [col for col in df_model.columns if col.startswith(\"LOCATIONN_\")]\n",
        "\n",
        "# Limpieza y separaci√≥n\n",
        "X = df_model[features]\n",
        "y = df_model[\"LOG_PRICE\"]\n",
        "model_data = pd.concat([X, y], axis=1).dropna()\n",
        "X_clean = model_data.drop(columns=[\"LOG_PRICE\"])\n",
        "y_clean = model_data[\"LOG_PRICE\"]\n",
        "\n",
        "# Divisi√≥n y modelo\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluaci√≥n\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Coeficientes ordenados por impacto\n",
        "coef_df = pd.DataFrame({\n",
        "    \"Variable\": X_train.columns,\n",
        "    \"Coeficiente\": model.coef_\n",
        "})\n",
        "coef_df[\"ImpactoAbs\"] = coef_df[\"Coeficiente\"].abs()\n",
        "coef_df_sorted = coef_df.sort_values(by=\"ImpactoAbs\", ascending=False)\n",
        "\n",
        "rmse, r2, coef_df_sorted.head(20)\n"
      ],
      "metadata": {
        "id": "YM3TjJdemHmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizaci√≥n: Precio real vs Predicho (con zona incluida)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Convertimos de log10 a escala real\n",
        "y_pred_real = 10 ** y_pred\n",
        "y_test_real = 10 ** y_test\n",
        "\n",
        "# Gr√°fico\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=y_test_real, y=y_pred_real, alpha=0.3)\n",
        "plt.plot([y_test_real.min(), y_test_real.max()],\n",
        "         [y_test_real.min(), y_test_real.max()],\n",
        "         'r--')\n",
        "plt.xlabel(\"Precio real (‚Ç¨)\")\n",
        "plt.ylabel(\"Precio predicho (‚Ç¨)\")\n",
        "plt.title(\"Precio real vs. predicho (modelo lineal con zona)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "urWykSyPnYla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos un modelo Random Forest Regressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Usamos los mismos datos ya preparados\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predicciones y evaluaci√≥n\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "rmse_rf, r2_rf\n"
      ],
      "metadata": {
        "id": "FK0_cGXknfAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gr√°fico de dispersi√≥n: Precio real vs. Precio predicho para Random Forest\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=10 ** y_test, y=10 ** y_pred_rf, alpha=0.3)\n",
        "plt.plot([10 ** y_test.min(), 10 ** y_test.max()],\n",
        "         [10 ** y_test.min(), 10 ** y_test.max()],\n",
        "         'r--')\n",
        "plt.xlabel(\"Precio real (‚Ç¨)\")\n",
        "plt.ylabel(\"Precio predicho (‚Ç¨)\")\n",
        "plt.title(\"Precio real vs. predicho (Random Forest)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VoNObzwSnxB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizamos la importancia de las variables en Random Forest\n",
        "importances = rf_model.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# Creamos un DataFrame ordenado\n",
        "importance_df = pd.DataFrame({\n",
        "    \"Variable\": feature_names,\n",
        "    \"Importancia\": importances\n",
        "}).sort_values(by=\"Importancia\", ascending=False)\n",
        "\n",
        "# Mostramos el top 20\n",
        "top_20 = importance_df.head(20)\n",
        "\n",
        "# Gr√°fico\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=top_20, y=\"Variable\", x=\"Importancia\", palette=\"viridis\")\n",
        "plt.title(\"Importancia de variables (Random Forest)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "top_20\n"
      ],
      "metadata": {
        "id": "MXV-d0jDn48d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Creamos una funci√≥n para entrenar y evaluar modelos\n",
        "def evaluar_modelo(modelo, X_train, X_test, y_train, y_test, nombre=\"Modelo\"):\n",
        "    modelo.fit(X_train, y_train)\n",
        "    pred = modelo.predict(X_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
        "    r2 = r2_score(y_test, pred)\n",
        "    return {\"Modelo\": nombre, \"RMSE\": rmse, \"R¬≤\": r2}\n",
        "\n",
        "# Escalamos los datos para SVM y KNN\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Modelos a evaluar\n",
        "modelos = [\n",
        "    (LinearRegression(), \"Regresi√≥n Lineal\"),\n",
        "    (Ridge(alpha=1.0), \"Ridge\"),\n",
        "    (GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=42), \"Gradient Boosting\"),\n",
        "    (SVR(), \"SVM (SVR)\"),\n",
        "    (KNeighborsRegressor(n_neighbors=5), \"KNN (k=5)\"),\n",
        "    (RandomForestRegressor(n_estimators=100, random_state=42), \"Random Forest\")\n",
        "]\n",
        "\n",
        "# Evaluamos los modelos\n",
        "resultados = []\n",
        "for modelo, nombre in modelos:\n",
        "    if nombre in [\"SVM (SVR)\", \"KNN (k=5)\"]:\n",
        "        res = evaluar_modelo(modelo, X_train_scaled, X_test_scaled, y_train, y_test, nombre)\n",
        "    else:\n",
        "        res = evaluar_modelo(modelo, X_train, X_test, y_train, y_test, nombre)\n",
        "    resultados.append(res)\n",
        "\n",
        "# Mostramos los resultados ordenados por R¬≤\n",
        "import pandas as pd\n",
        "df_resultados = pd.DataFrame(resultados).sort_values(by=\"R¬≤\", ascending=False)\n",
        "# Mostrar el DataFrame con los resultados sin usar ace_tools\n",
        "df_resultados.reset_index(drop=True, inplace=True)\n",
        "df_resultados.style.format({\"RMSE\": \"{:,.0f}\", \"R¬≤\": \"{:.3f}\"})\n",
        "\n"
      ],
      "metadata": {
        "id": "KLVtM5W8oJf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üì¶ Carga de librer√≠as\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# üöÄ Carga de datos\n",
        "df = pd.read_csv(\"/content/Madrid_limpio_con_zona.csv\")\n",
        "\n",
        "# üßº Selecci√≥n y limpieza de variables\n",
        "features = [\n",
        "    'CONSTRUCTEDAREA', 'ROOMNUMBER', 'BATHNUMBER', 'FLOORCLEAN', 'HASLIFT',\n",
        "    'HASTERRACE', 'HASAIRCONDITIONING', 'HASPARKINGSPACE', 'HASWARDROBE',\n",
        "    'HASDOORMAN', 'HASGARDEN', 'HASSWIMMINGPOOL', 'DISTANCE_TO_CITY_CENTER',\n",
        "    'ANTIGUEDAD'\n",
        "]\n",
        "df_model = df[features + [\"PRICE\"]].dropna()\n",
        "X = df_model[features]\n",
        "y = np.log10(df_model[\"PRICE\"])\n",
        "\n",
        "# ‚úÇÔ∏è Divisi√≥n en train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# üìè Escalado para SVM y KNN\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ü§ñ Modelos a evaluar\n",
        "modelos = [\n",
        "    (LinearRegression(), \"Regresi√≥n Lineal\"),\n",
        "    (Ridge(alpha=1.0), \"Ridge\"),\n",
        "    (GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=42), \"Gradient Boosting\"),\n",
        "    (SVR(), \"SVM (SVR)\"),\n",
        "    (KNeighborsRegressor(n_neighbors=5), \"KNN (k=5)\"),\n",
        "    (RandomForestRegressor(n_estimators=100, random_state=42), \"Random Forest\")\n",
        "]\n",
        "\n",
        "# üìä Evaluaci√≥n y almacenamiento\n",
        "resultados = []\n",
        "modelos_entrenados = {}\n",
        "\n",
        "for modelo, nombre in modelos:\n",
        "    if nombre in [\"SVM (SVR)\", \"KNN (k=5)\"]:\n",
        "        modelo.fit(X_train_scaled, y_train)\n",
        "        pred = modelo.predict(X_test_scaled)\n",
        "    else:\n",
        "        modelo.fit(X_train, y_train)\n",
        "        pred = modelo.predict(X_test)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
        "    r2 = r2_score(y_test, pred)\n",
        "    resultados.append({\"Modelo\": nombre, \"RMSE\": rmse, \"R¬≤\": r2})\n",
        "    modelos_entrenados[nombre] = modelo\n",
        "\n",
        "# üìã Mostrar resultados\n",
        "df_resultados = pd.DataFrame(resultados).sort_values(by=\"R¬≤\", ascending=False).reset_index(drop=True)\n",
        "df_resultados\n"
      ],
      "metadata": {
        "id": "IkazyY9pr7lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Definimos un grid reducido de b√∫squeda\n",
        "param_distributions = {\n",
        "    'C': [1, 10],\n",
        "    'epsilon': [0.1, 1],\n",
        "    'kernel': ['rbf']  # El kernel 'linear' suele rendir peor para datos no lineales\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV: eval√∫a 4 combinaciones al azar con validaci√≥n cruzada\n",
        "random_svr = RandomizedSearchCV(\n",
        "    SVR(),\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=4,           # Solo 4 combinaciones\n",
        "    scoring='r2',\n",
        "    cv=2,               # Validaci√≥n cruzada de 2 folds\n",
        "    verbose=1,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Entrenamos (usa los datos escalados)\n",
        "random_svr.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Mejor modelo\n",
        "best_svr = random_svr.best_estimator_\n",
        "print(\"Mejor combinaci√≥n de hiperpar√°metros:\", random_svr.best_params_)\n",
        "\n",
        "# Predicciones y m√©tricas\n",
        "y_pred_svr = best_svr.predict(X_test_scaled)\n",
        "r2 = r2_score(y_test, y_pred_svr)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred_svr))\n",
        "\n",
        "print(\"R¬≤:\", round(r2, 3))\n",
        "print(\"RMSE:\", round(rmse, 3))\n"
      ],
      "metadata": {
        "id": "d4HCUI0FuXhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# Creamos un dataframe solo con columnas num√©ricas ya transformadas\n",
        "df_model = df[[\n",
        "    'CONSTRUCTEDAREA', 'ROOMNUMBER', 'BATHNUMBER', 'ANTIGUEDAD',\n",
        "    'HASLIFT', 'HASAIRCONDITIONING', 'HASPARKINGSPACE', 'HASGARDEN',\n",
        "    'HASSWIMMINGPOOL', 'DISTANCE_TO_CITY_CENTER', 'PRICE'\n",
        "]].dropna()\n",
        "\n",
        "# Transformaci√≥n del precio (log)\n",
        "df_model[\"log_price\"] = np.log10(df_model[\"PRICE\"])\n",
        "\n",
        "# Ajustamos regresi√≥n cuantil√≠lica para varios percentiles\n",
        "quantiles = [0.1, 0.5, 0.9]\n",
        "models = {}\n",
        "\n",
        "for q in quantiles:\n",
        "    mod = smf.quantreg(\"log_price ~ CONSTRUCTEDAREA + ROOMNUMBER + BATHNUMBER + ANTIGUEDAD + HASLIFT + HASAIRCONDITIONING + HASPARKINGSPACE + HASGARDEN + HASSWIMMINGPOOL + DISTANCE_TO_CITY_CENTER\", df_model)\n",
        "    res = mod.fit(q=q)\n",
        "    models[q] = res\n",
        "    print(f\"\\n----- Cuantil {q} -----\")\n",
        "    print(res.summary())\n"
      ],
      "metadata": {
        "id": "BltHdDQH9HYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "coef_dict = {var: [] for var in models[0.1].params.index}\n",
        "\n",
        "for q in quantiles:\n",
        "    for var in coef_dict:\n",
        "        coef_dict[var].append(models[q].params[var])\n",
        "\n",
        "# Gr√°fico de coeficientes\n",
        "for var, coef_list in coef_dict.items():\n",
        "    if var != \"Intercept\":\n",
        "        plt.plot(quantiles, coef_list, marker='o', label=var)\n",
        "\n",
        "plt.xlabel(\"Cuantil\")\n",
        "plt.ylabel(\"Coeficiente\")\n",
        "plt.title(\"Comparaci√≥n de coeficientes por cuantil\")\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xPvQcLUm9VtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los resultados revelan patrones muy interesantes:\n",
        "\n",
        "üü™ HASLIFT (ascensor) muestra un coeficiente positivo y creciente hacia los cuantiles superiores, lo que indica que su efecto es m√°s relevante en viviendas caras. Esto sugiere que la presencia de ascensor puede tener un mayor impacto en el valor de propiedades de gama alta, posiblemente por estar ubicadas en edificios altos o premium.\n",
        "\n",
        "üü® HASAIRCONDITIONING (aire acondicionado) tambi√©n presenta una influencia positiva estable, aunque ligeramente decreciente, lo que indica que es valorado en todos los segmentos, pero especialmente en los intermedios.\n",
        "\n",
        "üü© BATHNUMBER mantiene un impacto positivo moderado y estable en toda la distribuci√≥n, lo que valida su importancia como predictor del precio.\n",
        "\n",
        "üü• ANTIGUEDAD (a√±os desde construcci√≥n) tiene un efecto ligeramente negativo y relativamente constante en todos los cuantiles, como era de esperar: a mayor antig√ºedad, menor valor.\n",
        "\n",
        "üîµ DISTANCE_TO_CITY_CENTER tiene un coeficiente negativo creciente, lo que sugiere que la localizaci√≥n central es m√°s decisiva en el segmento alto del mercado (cuantil 0.9), donde los compradores valoran mucho la proximidad al centro urbano.\n",
        "\n",
        "üüß ROOMNUMBER cambia de signo a lo largo de los cuantiles, lo que sugiere que su efecto no es lineal ni uniforme: en viviendas de bajo coste puede no estar correlacionado con el precio (por ejemplo, en viviendas compartidas o reformadas), mientras que en viviendas caras pierde importancia frente a otras comodidades.\n",
        "\n",
        "üü´ Variables como HASPARKINGSPACE o HASSWIMMINGPOOL muestran una ligera disminuci√≥n en el efecto hacia los cuantiles altos, lo que podr√≠a deberse a que ya est√°n impl√≠citas en la gama alta y por tanto no a√±aden valor marginal adicional.\n",
        "\n",
        "üß† Interpretaci√≥n general\n",
        "Este an√°lisis demuestra que el impacto de ciertos atributos var√≠a a lo largo del espectro de precios. Variables como el ascensor o la distancia al centro tienen mayor peso en viviendas caras, mientras que caracter√≠sticas como el n√∫mero de habitaciones no muestran un patr√≥n claro. Este tipo de hallazgos no solo mejora la comprensi√≥n del mercado, sino que tambi√©n puede ayudar a ajustar los modelos predictivos, por ejemplo, mediante t√©cnicas de modelado segmentado o dise√±o de submodelos por rangos de precio.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "n√°lisis m√°s visual e intuitivo: el mapa de errores de predicci√≥n. Este paso es ideal para demostrar la capacidad espacial del modelo y detectar si hay zonas donde sobrevalora o infravalora los inmuebles.\n",
        "\n",
        "üó∫Ô∏è Mapa de errores de predicci√≥n con Folium (en Google Colab o Jupyter)\n",
        "Este c√≥digo asume que ya tienes:\n",
        "\n",
        "Un modelo entrenado (por ejemplo, Random Forest)\n",
        "\n",
        "Predicciones en log(PRICE)\n",
        "\n",
        "Coordenadas (LATITUDE, LONGITUDE)\n",
        "\n",
        "El dataframe original df\n",
        "\n"
      ],
      "metadata": {
        "id": "vkPKwAJi9hDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import folium\n",
        "from folium.plugins import MarkerCluster\n",
        "\n",
        "# 1. Creamos copia del set de test\n",
        "df_resultados = X_test.copy()\n",
        "df_resultados[\"price_real\"] = y_test.values\n",
        "\n",
        "# 2. Generamos predicciones en euros (no log)\n",
        "modelo = modelos_entrenados[\"Random Forest\"]  # O cualquier otro\n",
        "df_resultados[\"price_pred\"] = modelo.predict(X_test)\n",
        "\n",
        "# 3. Calculamos errores\n",
        "df_resultados[\"error_abs\"] = df_resultados[\"price_pred\"] - df_resultados[\"price_real\"]\n",
        "df_resultados[\"error_pct\"] = df_resultados[\"error_abs\"] / df_resultados[\"price_real\"]\n",
        "\n",
        "# 4. A√±adimos coordenadas desde el dataframe original\n",
        "df_resultados[\"LATITUDE\"] = df.loc[df_resultados.index, \"LATITUDE\"]\n",
        "df_resultados[\"LONGITUDE\"] = df.loc[df_resultados.index, \"LONGITUDE\"]\n",
        "\n",
        "# 5. Mapa interactivo\n",
        "m = folium.Map(location=[40.4168, -3.7038], zoom_start=11)\n",
        "cluster = MarkerCluster().add_to(m)\n",
        "\n",
        "for _, row in df_resultados.iterrows():\n",
        "    if pd.notnull(row[\"LATITUDE\"]) and pd.notnull(row[\"LONGITUDE\"]):\n",
        "        color = (\n",
        "            \"green\" if row[\"error_pct\"] < -0.25 else\n",
        "            \"red\" if row[\"error_pct\"] > 0.25 else\n",
        "            \"blue\"\n",
        "        )\n",
        "        popup_text = (\n",
        "            f\"<b>Real:</b> {round(row['price_real']):,} ‚Ç¨<br>\"\n",
        "            f\"<b>Predicho:</b> {round(row['price_pred']):,} ‚Ç¨<br>\"\n",
        "            f\"<b>Error:</b> {round(row['error_abs']):,} ‚Ç¨ \"\n",
        "            f\"({round(row['error_pct'] * 100, 1)}%)\"\n",
        "        )\n",
        "        folium.CircleMarker(\n",
        "            location=[row[\"LATITUDE\"], row[\"LONGITUDE\"]],\n",
        "            radius=3,\n",
        "            fill=True,\n",
        "            color=color,\n",
        "            fill_opacity=0.5,\n",
        "            popup=popup_text,\n",
        "        ).add_to(cluster)\n",
        "\n",
        "m\n"
      ],
      "metadata": {
        "id": "Fjdr0Oy29y5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. A√±adir la zona (barrio o distrito) a los resultados\n",
        "df_resultados[\"LOCATIONN\"] = df.loc[df_resultados.index, \"LOCATIONN\"]\n",
        "\n",
        "# 2. Agrupar por zona\n",
        "df_zonas = df_resultados.groupby(\"LOCATIONN\").agg(\n",
        "    precio_real_medio=(\"price_real\", \"mean\"),\n",
        "    precio_predicho_medio=(\"price_pred\", \"mean\"),\n",
        "    error_medio=(\"error_abs\", \"mean\"),\n",
        "    error_pct_medio=(\"error_pct\", \"mean\"),\n",
        "    n_observaciones=(\"price_real\", \"count\")\n",
        ").reset_index()\n",
        "\n",
        "# 3. Ordenar por mayor error porcentual\n",
        "df_zonas = df_zonas.sort_values(by=\"error_pct_medio\", ascending=False)\n",
        "\n",
        "# 4. Mostrar tabla\n",
        "import pandas as pd\n",
        "import IPython.display as display\n",
        "\n",
        "display.display(df_zonas.head(10))\n"
      ],
      "metadata": {
        "id": "Mg4gwoflAsM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "\n",
        "# Cargar shapefile si no lo tienes en memoria\n",
        "# Si ya lo tienes, salta esta l√≠nea\n",
        "Madrid_Polygons = gpd.read_file(\"/content/Madrid_Polygons.shp\")  # ajusta la ruta si es necesario\n",
        "# Aseg√∫rate de que el nombre de zona coincida: LOCATIONN\n",
        "Madrid_Polygons = Madrid_Polygons.copy()\n",
        "Madrid_Polygons[\"LOCATIONN\"] = Madrid_Polygons[\"LOCATIONN\"].str.strip()\n",
        "\n",
        "# Unimos con el resumen por zona\n",
        "gdf = Madrid_Polygons.merge(df_zonas, on=\"LOCATIONN\", how=\"left\")\n",
        "import folium\n",
        "from branca.colormap import linear\n",
        "\n",
        "# Mapa base\n",
        "m = folium.Map(location=[40.4168, -3.7038], zoom_start=11)\n",
        "\n",
        "# Colormap para error porcentual\n",
        "colormap = linear.RdYlGn_11.scale(gdf[\"error_pct_medio\"].min(), gdf[\"error_pct_medio\"].max())\n",
        "colormap.caption = \"Error porcentual medio (predicho - real)\"\n",
        "colormap.add_to(m)\n",
        "\n",
        "# Pintamos cada zona con color seg√∫n error\n",
        "for _, row in gdf.iterrows():\n",
        "    if pd.notnull(row[\"error_pct_medio\"]):\n",
        "        color = colormap(row[\"error_pct_medio\"])\n",
        "        popup = folium.Popup(f\"\"\"\n",
        "            <b>{row['LOCATIONN']}</b><br>\n",
        "            <b>Precio real medio:</b> {round(row['precio_real_medio']):,} ‚Ç¨<br>\n",
        "            <b>Precio predicho medio:</b> {round(row['precio_predicho_medio']):,} ‚Ç¨<br>\n",
        "            <b>Error:</b> {round(row['error_pct_medio'] * 100, 1)}%<br>\n",
        "            <b>Observaciones:</b> {row['n_observaciones']}\n",
        "        \"\"\", max_width=300)\n",
        "        geojson = folium.GeoJson(row[\"geometry\"], style_function=lambda x, color=color: {\n",
        "            \"fillColor\": color,\n",
        "            \"color\": \"black\",\n",
        "            \"weight\": 0.5,\n",
        "            \"fillOpacity\": 0.7\n",
        "        })\n",
        "        geojson.add_child(popup)\n",
        "        geojson.add_to(m)\n",
        "\n",
        "m\n"
      ],
      "metadata": {
        "id": "eBZKXoj3BW8_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}